{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670c2555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vkarthikeyan1/reddit_health/health_subreddit_small/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, Dataset\n",
    "# Registers the factory\n",
    "from scispacy.linking import EntityLinker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d729fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\"\n",
    "BATCH_SIZE = 1024       # GPU Batch size\n",
    "INDEX_CHUNK_SIZE = 50000 # RAM Batch size\n",
    "TOP_K = 1\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "FAISS_INDEX_STRING = \"IVF1024,PQ64\" \n",
    "\n",
    "print(f\"Running on: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdf315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vkarthikeyan1/reddit_health/.venv/lib/python3.10/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
      "/home/vkarthikeyan1/reddit_health/.venv/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/vkarthikeyan1/reddit_health/.venv/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ UMLS KB Loaded. Total concepts: 3920422\n",
      "2. Loading SapBERT Model...\n",
      "   ✅ SapBERT Loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "kb = linker.kb\n",
    "print(f\"   ✅ UMLS KB Loaded. Total concepts: {len(kb.cui_to_entity)}\")\n",
    "\n",
    "print(\"2. Loading SapBERT Model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "print(\"   ✅ SapBERT Loaded.\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d0074b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUI: C0000005, Name: (131)I-Macroaggregated Albumin\n",
       "Definition: None\n",
       "TUI(s): T116, T121, T130\n",
       "Aliases: (total: 1): \n",
       "\t (131)I-MAA"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.cui_to_entity['C0000005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a524e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n",
      "/home/vkarthikeyan1/reddit_health/.venv/lib/python3.10/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
      "/home/vkarthikeyan1/reddit_health/.venv/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/vkarthikeyan1/reddit_health/.venv/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  cancer\n",
      "CUI: C0006826, Name: Malignant Neoplasms\n",
      "Definition: A tumor composed of atypical neoplastic, often pleomorphic cells that invade other tissues. Malignant neoplasms often metastasize to distant anatomic sites and may recur after excision. The most common malignant neoplasms are carcinomas, Hodgkin and non-Hodgkin lymphomas, leukemias, melanomas, and sarcomas.\n",
      "TUI(s): T191\n",
      "Aliases (abbreviated, total: 44): \n",
      "\t Neoplasms, Malignant, Cancers, Tumor, malignant, NOS, Malignant neoplasm without specification of site, Neoplastic disease, malignant, CA - Cancer, Malignancy, unspecified site, neoplasm/cancer, malignant neoplasm, Tumor, malignant\n",
      "CUI: C0998265, Name: Cancer Genus\n",
      "Definition: None\n",
      "TUI(s): T204\n",
      "Aliases: (total: 2): \n",
      "\t Cancer, crab\n",
      "CUI: C1306459, Name: Primary malignant neoplasm\n",
      "Definition: A malignant tumor at the original site of growth.\n",
      "TUI(s): T191\n",
      "Aliases (abbreviated, total: 19): \n",
      "\t Malignant neoplasm, primary (morphologic abnormality), malignant neoplasm, Tumor, malignant, Malignancy, Malignant neoplasm, Malignant tumor morphology, Tumour, malignant, Primary malignant neoplasm (disorder), Unclassified tumor, malignant, Primary malignant neoplasm\n",
      "CUI: C1547140, Name: Specialty Type - cancer\n",
      "Definition: None\n",
      "TUI(s): T091\n",
      "Aliases: (total: 1): \n",
      "\t Cancer\n",
      "CUI: C2707253, Name: Cancer:-:Point in time:^Patient:-\n",
      "Definition: None\n",
      "TUI(s): T201\n",
      "Aliases: (total: 3): \n",
      "\t Cancer, Cancer:-:Pt:^Patient:-, Cancer:-:To identify measures at a point in time:^Patient:-\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "# This line takes a while, because we have to download ~1GB of data\n",
    "# and load a large JSON file (the knowledge base). Be patient!\n",
    "# Thankfully it should be faster after the first time you use it, because\n",
    "# the downloads are cached.\n",
    "# NOTE: The resolve_abbreviations parameter is optional, and requires that\n",
    "# the AbbreviationDetector pipe has already been added to the pipeline. Adding\n",
    "# the AbbreviationDetector pipe and setting resolve_abbreviations to True means\n",
    "# that linking will only be performed on the long form of abbreviations.\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "\n",
    "doc = nlp(\"Smoking cures cancer\")\n",
    "\n",
    "# Let's look at a random entity!\n",
    "entity = doc.ents[1]\n",
    "\n",
    "print(\"Name: \", entity)\n",
    "\n",
    "\n",
    "# Each entity is linked to UMLS with a score\n",
    "# (currently just char-3gram matching).\n",
    "linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "for umls_ent in entity._.kb_ents:\n",
    "\tprint(linker.kb.cui_to_entity[umls_ent[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cce7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "ds_triples = load_from_disk(\"pubmed_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81318a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "claim\n",
       "na           15941\n",
       "ambiguous     1098\n",
       "true           204\n",
       "false           49\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=ds_triples.to_pandas()\n",
    "df['claim'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18dae272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298                    [('cipro', 'causes', 'tendonitis')]\n",
       "508      [('divalproex', 'linked_to', 'violent deaths')...\n",
       "1031     [('meniscus', 'heals', 'young age'), ('meniscu...\n",
       "1274     [('metronidazole', 'treats', 'anaerobic gut fl...\n",
       "1960     [('dairy', 'causes', 'acne'), ('egg whites', '...\n",
       "2357     [('medication', 'causes', 'sensation in limbs'...\n",
       "2422     [('MRSA', 'treats', 'clindamycin'), ('MRSA', '...\n",
       "2682     [('eosinophilic oesophagitis', 'causes', 'diff...\n",
       "3066     [('celiac disease', 'treats', 'gluten free die...\n",
       "3325     [('endometriosis', 'treats', 'surgery'), ('end...\n",
       "3424                  [('toothache', 'treats', 'dentist')]\n",
       "3438                   [('bronchitis', 'treats', 'strep')]\n",
       "3487     [('stress', 'causes', 'heart palpitations'), (...\n",
       "3685     [('celiac disease', 'treats', 'gluten-free die...\n",
       "4518     [('it', 'cures', 'cancer'), ('it', 'cures', 'v...\n",
       "4632                   [('antibiotic', 'causes', 'cough')]\n",
       "4639                   [('antibiotic', 'causes', 'cough')]\n",
       "4819     [('gene therapy', 'treats', 'retinitis pigment...\n",
       "5431     [('celiac disease', 'causes', 'severe health i...\n",
       "5925     [('leech therapy', 'treats', 'pain'), ('leech ...\n",
       "6283     [('posterior tibial nerve stimulation', 'treat...\n",
       "6481       [('depression', 'treats', 'positive activity')]\n",
       "6510                     [('abscess', 'treats', 'doctor')]\n",
       "7648      [('addiction', 'treats', 'healthy alternative')]\n",
       "8530     [('minimum wage laws', 'causes', 'unemployment...\n",
       "8699            [('hip dysplasia', 'treats', 'expensive')]\n",
       "8815     [('pelvic floor physical therapy', 'treats', '...\n",
       "8986     [('exercise', 'prevents', 'overweight'), ('exe...\n",
       "9242     [('migraine medication', 'causes', 'headache')...\n",
       "9330     [('bacterial sinus infection', 'confused_with'...\n",
       "9431     [('acne treatment', 'causes', 'liver issues'),...\n",
       "9690                        [('wine', 'causes', 'nausea')]\n",
       "9904     [('neem oil', 'treats', 'fungus'), ('neem oil'...\n",
       "10567    [('Cognitive behavioral therapy', 'treats', 'a...\n",
       "10832    [('prozac', 'treats', 'depression'), ('prozac'...\n",
       "10895    [('gene therapy for hearing', 'treats', 'heari...\n",
       "10908    [(\"Hashimoto's thyroiditis\", 'causes', 'chroni...\n",
       "10934    [('vitamin d deficiency', 'causes', 'poor slee...\n",
       "11964    [('strep throat', 'treats', 'rheumatic fever')...\n",
       "13496    [('sleep apnea', 'treats', 'CPAP machine'), ('...\n",
       "14415    [('ibs symptoms', 'associated_with', 'celiac d...\n",
       "14530                    [('vitamin D', 'treats', 'this')]\n",
       "14566    [('sialadenitis', 'treats', 'warm compresses')...\n",
       "14793    [('collarbone area', 'pops_like_knuckle', 'pai...\n",
       "14957    [('vitamin d test', 'used_for', 'diagnosing vi...\n",
       "15274    [('vitamin D deficiency', 'treats', 'ailments'...\n",
       "15713    [('diabetes', 'associated_with', 'kidney damag...\n",
       "16749    [('Benadryl', 'treats', 'sinus issues'), ('Ben...\n",
       "17006    [('ibuprofen', 'treats', 'pain'), ('ibuprofen'...\n",
       "Name: triples, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"triples\"][df['claim']==\"false\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f797426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_df = pd.read_csv(\"health_misinfo_speed_run.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3358119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author              100000\n",
       "body                100000\n",
       "controversiality    100000\n",
       "created_utc         100000\n",
       "link_id             100000\n",
       "score               100000\n",
       "subreddit           100000\n",
       "subreddit_id        100000\n",
       "id                  100000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe47beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "df_new= pd.read_parquet(\"triples_output_100k.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5cf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "df_new[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25405a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  3.57ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 42.0MB / 42.0MB, 30.0MB/s  \n",
      "New Data Upload: 100%|██████████| 42.0MB / 42.0MB, 30.0MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.63s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Vijayrathank/reddit-health-triples-lg/commit/ef9a057030af069df9d1d97d9cc5c39a4d821fb9', commit_message='Upload dataset', commit_description='', oid='ef9a057030af069df9d1d97d9cc5c39a4d821fb9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Vijayrathank/reddit-health-triples-lg', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Vijayrathank/reddit-health-triples-lg'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds=Dataset.from_pandas(df_new)\n",
    "ds.push_to_hub(\"Vijayrathank/reddit-health-triples-lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22985360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_cui= pd.read_parquet(\"../semmed_verifier/triples_cui_mapped.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5a3f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def has_triples(val):\n",
    "    if val is None: return False\n",
    "    s_val = str(val).strip()\n",
    "    return s_val != \"[]\" and s_val != \"\" and s_val != \"None\"\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "df_filtered = df_cui[df_cui['triples'].apply(has_triples)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f514c245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21777"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered['triples'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af9ac92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  3.37ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 43.3MB / 43.3MB, 1.40MB/s  \n",
      "New Data Upload: 100%|██████████| 1.37MB / 1.37MB, 1.40MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.76s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Vijayrathank/reddit-health-triples-lg/commit/6ac65b746d489dc068934ed544ecfca031455acb', commit_message='Upload dataset', commit_description='', oid='6ac65b746d489dc068934ed544ecfca031455acb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Vijayrathank/reddit-health-triples-lg', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Vijayrathank/reddit-health-triples-lg'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds=Dataset.from_pandas(df_cui)\n",
    "ds.push_to_hub(\"Vijayrathank/reddit-health-triples-lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3cfe114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ground= pd.read_parquet(\"../ground_truth/reddit_health_ground_truth.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff55e4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>triples</th>\n",
       "      <th>ground_truth_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(\"Ovulation\", \"is_associated_with\", \"Fertilit...</td>\n",
       "      <td>SUPPORTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(\"Load shedding\", \"prevents\", \"Power outage\")]</td>\n",
       "      <td>UNVERIFIED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(\"Shedding\", \"is_normal\", \"Hair loss\")]</td>\n",
       "      <td>AMBIGUOUS_SUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(\"Argan oil\", \"improves\", \"Skin hydration\"), ...</td>\n",
       "      <td>AMBIGUOUS_SUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(\"Evergreen tree\", \"is_associated_with\", \"Sno...</td>\n",
       "      <td>SUPPORTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21772</th>\n",
       "      <td>[(\"Shedding\", \"prevents\", \"Hair matting\"), (\"S...</td>\n",
       "      <td>AMBIGUOUS_SUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21773</th>\n",
       "      <td>[(\"Glyphosate\", \"is_more_toxic_than\", \"Caffein...</td>\n",
       "      <td>AMBIGUOUS_SUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21774</th>\n",
       "      <td>[(\"Dogs\", \"can_lead_to\", \"Diarrhea\"), (\"Dogs\",...</td>\n",
       "      <td>UNVERIFIED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21775</th>\n",
       "      <td>[(\"Herpes viroids\", \"manifests_as\", \"Sore\"), (...</td>\n",
       "      <td>SUPPORTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21776</th>\n",
       "      <td>[(\"Shedding\", \"manifests_as\", \"Viral transmiss...</td>\n",
       "      <td>SUPPORTED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21777 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 triples ground_truth_label\n",
       "0      [(\"Ovulation\", \"is_associated_with\", \"Fertilit...          SUPPORTED\n",
       "1        [(\"Load shedding\", \"prevents\", \"Power outage\")]         UNVERIFIED\n",
       "2               [(\"Shedding\", \"is_normal\", \"Hair loss\")]  AMBIGUOUS_SUPPORT\n",
       "3      [(\"Argan oil\", \"improves\", \"Skin hydration\"), ...  AMBIGUOUS_SUPPORT\n",
       "4      [(\"Evergreen tree\", \"is_associated_with\", \"Sno...          SUPPORTED\n",
       "...                                                  ...                ...\n",
       "21772  [(\"Shedding\", \"prevents\", \"Hair matting\"), (\"S...  AMBIGUOUS_SUPPORT\n",
       "21773  [(\"Glyphosate\", \"is_more_toxic_than\", \"Caffein...  AMBIGUOUS_SUPPORT\n",
       "21774  [(\"Dogs\", \"can_lead_to\", \"Diarrhea\"), (\"Dogs\",...         UNVERIFIED\n",
       "21775  [(\"Herpes viroids\", \"manifests_as\", \"Sore\"), (...          SUPPORTED\n",
       "21776  [(\"Shedding\", \"manifests_as\", \"Viral transmiss...          SUPPORTED\n",
       "\n",
       "[21777 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ground[['triples','ground_truth_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "982c4b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Full Dataset Size: 86539\n",
      "Ground Truth Size: 21777\n",
      "Merging data...\n",
      "Filling missing labels for rows without triples...\n",
      "Saving combined dataset to ../ground_truth/reddit_health_full_annotated.parquet...\n",
      "Done!\n",
      "\n",
      "Label Distribution:\n",
      "ground_truth_label\n",
      "UNVERIFIED           71398\n",
      "AMBIGUOUS_SUPPORT     8821\n",
      "SUPPORTED             6320\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "FULL_DATASET_PATH = \"../dataset/100k_dataset/triples_output_100k.parquet\"       # The original data with all 100k rows\n",
    "GROUND_TRUTH_PATH = \"../ground_truth/reddit_health_ground_truth.parquet\" # The subset output from your Llama script\n",
    "OUTPUT_PATH = \"../ground_truth/reddit_health_full_annotated.parquet\"\n",
    "\n",
    "def main():\n",
    "    print(\"Loading datasets...\")\n",
    "    df_full = pd.read_parquet(FULL_DATASET_PATH)\n",
    "    df_gt = pd.read_parquet(GROUND_TRUTH_PATH)\n",
    "    \n",
    "    print(f\"Full Dataset Size: {len(df_full)}\")\n",
    "    print(f\"Ground Truth Size: {len(df_gt)}\")\n",
    "\n",
    "    # 1. Select only the necessary columns from Ground Truth to avoid duplicates\n",
    "    # We only need the ID to match, and the new label columns\n",
    "    cols_to_merge = ['id', 'ground_truth_label', 'ground_truth_explanation']\n",
    "    \n",
    "    # Safety check: Ensure 'id' exists in both\n",
    "    if 'id' not in df_full.columns or 'id' not in df_gt.columns:\n",
    "        raise ValueError(\"Error: 'id' column missing. Cannot merge reliably.\")\n",
    "\n",
    "    # 2. Perform Left Merge\n",
    "    # how='left' keeps ALL rows from df_full, and attaches labels where ID matches\n",
    "    print(\"Merging data...\")\n",
    "    df_merged = df_full.merge(\n",
    "        df_gt[cols_to_merge], \n",
    "        on='id', \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # 3. Handle rows that were skipped (No Triples)\n",
    "    # If the extraction model found NO triples, the Llama judge didn't see it.\n",
    "    # Logically, if there are no triples, the pipeline prediction is \"UNVERIFIED\".\n",
    "    # We can fill NaNs with \"UNVERIFIED\" (or distinct \"NO_EXTRACTED_CLAIMS\" if you prefer).\n",
    "    \n",
    "    print(\"Filling missing labels for rows without triples...\")\n",
    "    values = {\"ground_truth_label\": \"UNVERIFIED\", \"ground_truth_explanation\": \"Skipped: No triples extracted.\"}\n",
    "    df_merged = df_merged.fillna(value=values)\n",
    "\n",
    "    # 4. Save\n",
    "    print(f\"Saving combined dataset to {OUTPUT_PATH}...\")\n",
    "    df_merged.to_parquet(OUTPUT_PATH, index=False)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    # Optional: Preview\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    print(df_merged['ground_truth_label'].value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5fafc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'body', 'controversiality', 'created_utc', 'link_id', 'score',\n",
       "       'subreddit', 'subreddit_id', 'id', '__index_level_0__', 'triples',\n",
       "       'ground_truth_label', 'ground_truth_explanation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotated= pd.read_parquet(\"../ground_truth/reddit_health_full_annotated.parquet\")\n",
    "\n",
    "df_annotated['ground_truth_label'].value_counts()\n",
    "df_annotated.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61ff46a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cui_triples', 'claim_verification', 'unified_claim_status'], dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_semmed= pd.read_parquet(\"../semmed_verifier/triples_semmed_unified_claims.parquet\")\n",
    "df_semmed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a2d1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def get_unified_status(verification_list):\n",
    "    \"\"\"\n",
    "    Aggregates micro-claims into a trinary macro-label for the comment.\n",
    "    Priority: SUPPORTED > AMBIGUOUS_SUPPORT > UNVERIFIED\n",
    "    \"\"\"\n",
    "    verification_list= ast.literal_eval(verification_list)\n",
    "    # 1. Handle empty/None\n",
    "    if not isinstance(verification_list, list) or not verification_list:\n",
    "        return \"UNVERIFIED\"\n",
    "    \n",
    "    # 2. Extract all statuses in this comment\n",
    "    statuses = set()\n",
    "    for item in verification_list:\n",
    "        # Get status, default to unverified if missing\n",
    "        s = item.get('claim_status', 'UNVERIFIED')\n",
    "        statuses.add(s)\n",
    "        \n",
    "    # 3. Hierarchy Check\n",
    "    \n",
    "    # Priority 1: If ANY verified fact exists, the comment is Supported.\n",
    "    if 'SUPPORTED' in statuses:\n",
    "        return \"SUPPORTED\"\n",
    "        \n",
    "    # Priority 2: If no strict support, but ambiguous/inverse evidence exists.\n",
    "    # We group INVERSE_SUPPORT here because it indicates a strong biological link \n",
    "    # (just the wrong direction), which fits \"Ambiguous\" better than \"Unverified\".\n",
    "    if 'AMBIGUOUS_SUPPORT' in statuses or 'INVERSE_SUPPORT' in statuses:\n",
    "        return \"AMBIGUOUS_SUPPORT\"\n",
    "        \n",
    "    # Priority 3: Fallback\n",
    "    return \"UNVERIFIED\"\n",
    "# --- Usage Example ---\n",
    "# Apply this to your dataframe after the previous step\n",
    "df_semmed['unified_claim_status'] = df_semmed['claim_verification'].apply(get_unified_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c8236ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unified_claim_status\n",
       "UNVERIFIED           79106\n",
       "AMBIGUOUS_SUPPORT     4247\n",
       "SUPPORTED             3186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_semmed['unified_claim_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a64d65d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ground_truth_label\n",
       "UNVERIFIED           71398\n",
       "AMBIGUOUS_SUPPORT     8821\n",
       "SUPPORTED             6320\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotated=pd.read_parquet(\"../ground_truth/reddit_health_full_annotated.parquet\")\n",
    "\n",
    "df_annotated['ground_truth_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb51596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scikit-Learn Evaluation ---\n",
      "Accuracy:  0.8398\n",
      "Precision: 0.7969\n",
      "Recall:    0.8398\n",
      "F1 Score:  0.8103\n",
      "\n",
      "--- Detailed Report ---\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "AMBIGUOUS_SUPPORT       0.37      0.18      0.24      8821\n",
      "        SUPPORTED       0.44      0.22      0.30      6320\n",
      "       UNVERIFIED       0.88      0.98      0.93     71398\n",
      "\n",
      "         accuracy                           0.84     86539\n",
      "        macro avg       0.56      0.46      0.49     86539\n",
      "     weighted avg       0.80      0.84      0.81     86539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def evaluate_model_sklearn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes metrics using scikit-learn.\n",
    "    y_true: List of ground truth values\n",
    "    y_pred: List of predicted values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Print results\n",
    "    print(\"--- Scikit-Learn Evaluation ---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Pro Tip: classification_report does all this in one go\n",
    "    print(\"\\n--- Detailed Report ---\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # 1 = Positive Class, 0 = Negative Class\n",
    "    df_annotated= pd.read_parquet(\"../ground_truth/reddit_health_full_annotated.parquet\")\n",
    "    df_semmed= pd.read_parquet(\"../semmed_verifier/triples_semmed_unified_claims.parquet\")\n",
    "    ground_truth = df_annotated['ground_truth_label']\n",
    "    predictions  = df_semmed['unified_claim_status']\n",
    "    \n",
    "    evaluate_model_sklearn(ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "777a9449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unified_claim_status\n",
       "UNVERIFIED           82777\n",
       "SUPPORTED             1893\n",
       "AMBIGUOUS_SUPPORT     1869\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "pubmed_output= pd.read_parquet(\"../pubmed_verifier/triples_pubmed_unified_claims.parquet\")\n",
    "pubmed_output['unified_claim_status'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3684ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your existing results\n",
    "input_file = \"triples_semmed_unified_claims.parquet\"\n",
    "output_file = \"triples_semmed_unified_claims_aligned.parquet\"\n",
    "\n",
    "print(f\"Loading {input_file}...\")\n",
    "pubmed_output= load_from_disk(\"../pubmed_verifier/pubmed_output\")\n",
    "\n",
    "df = pubmed_output.to_pandas()\n",
    "\n",
    "# 2. Check what current labels look like\n",
    "print(\"Original Label Distribution:\")\n",
    "print(df['unified_claim_status'].value_counts())\n",
    "\n",
    "# 3. Define the Mapping Dictionary\n",
    "label_map = {\n",
    "    \"true\": \"SUPPORTED\",\n",
    "    \"ambiguous\": \"AMBIGUOUS_SUPPORT\", \n",
    "    \"false\": \"UNVERIFIED\",  # Science contradicts claim -> Unverified/False Info\n",
    "    \"na\": \"UNVERIFIED\"      # No data -> Unverified\n",
    "}\n",
    "\n",
    "# 4. Apply the mapping\n",
    "# We use .get(x, x) to handle any unexpected labels gracefully (keeps them as is)\n",
    "df['unified_claim_status'] = df['unified_claim_status'].apply(lambda x: label_map.get(str(x).lower(), \"UNVERIFIED\"))\n",
    "\n",
    "# 5. Verify the new labels\n",
    "print(\"\\nNew Label Distribution:\")\n",
    "print(df['unified_claim_status'].value_counts())\n",
    "\n",
    "# 6. Save the updated file\n",
    "df.to_parquet(output_file)\n",
    "print(f\"\\nSaved aligned dataset to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89df4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ground_truth_label\n",
       "UNVERIFIED           71398\n",
       "AMBIGUOUS_SUPPORT     8821\n",
       "SUPPORTED             6320\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ground_truth=\"../ground_truth/reddit_health_full_annotated.parquet\"\n",
    "df_ground=pd.read_parquet(ground_truth)\n",
    "\n",
    "df_ground['ground_truth_label'].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7f4546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unified_claim_status\n",
       "UNVERIFIED           79106\n",
       "AMBIGUOUS_SUPPORT     4247\n",
       "SUPPORTED             3186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semmed_output= pd.read_parquet(\"../semmed_verifier/triples_semmed_unified_claims.parquet\")\n",
    "semmed_output['unified_claim_status'].value_counts()\n",
    "\n",
    "# pubmed_output= pd.read_parquet(\"./pubmed_verifier/triples_pubmed_unified_claims.parquet\")\n",
    "# pubmed_output['unified_claim_status'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff4055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(reddit)",
   "language": "python",
   "name": "reddit_health_v1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
